# Movie recommendation


![everything_is_reco_800px_web](https://user-images.githubusercontent.com/110397465/213392639-60a5186b-cd73-45ba-bf35-0d90d481d294.jpg)



# About DataSet

    Knowledge-based, Content-based and Collaborative systems are built on MovieLens dataset with 100,000 movie ratings. These Recommender systems were built using Pandas operations and by fitting KNN, SVD & deep learning models which use NLP techniques and NN architecture to suggest movies for the users based on similar users and for queries specific to genre, user, movie, rating, popularity.

    A recommender system is a subclass of information filtering system that seeks to predict the "rating" or "preference" a user would give to an item. Recommender systems are utilized in a variety of areas including movies, music, news, social tags, and products in general. Recommender systems typically produce a list of recommendations and there are few ways in which it can be done. Two of the most popular ways are â€“ through collaborative filtering or through content-based filtering.

    Most internet products we use today are powered by recommender systems. YouTube, Netflix, Amazon, Pinterest, and long list of other internet products all rely on recommender systems to filter millions of contents and make personalized recommendations to their users. Recommender systems are well-studied and proven to provide tremendous values to internet businesses and their consumers.


# Requirments:

    *  Clone this repository using git clone

    https://github.com/Prathima0808/Project-4.git


    *   Install the following libraries in your local using terminal/Gitbash. You'll use these libraries later in your code for the data analysis purpose.

        1. Pandas
        pip install pandas
        requests
        python -m pip install requests
        
        standard library of python so no need to download it

        2. sqlalchemy
        conda install -c anaconda sqlalchemy
        config
        pip install config

        3. Scikit-learn Mechiene Learning Library

        4. Additional: CSS, HTML, Python, Pandas, SKLearn, Bootstrap, SQLite, Jupyter Notebook



# Extracting, Transforming and Loading process (ETL)

    The data was provided to us in a CSV. We checked for any null values in the dataset, and did not have any. We also checked to verify that datatypes matched the variables values as described above. Our data checked out in all these areas, so no additional transformation was required. Then, we created an SQLite database for our data to be stored in. We did this using our knowledge of SQLite and Pandas.

    In this process we're extracting data into dataframe:

    





# Exploratory Data Analysis:



    






   





    





